# ğŸ”„ Training Pipeline Workflow

## Visual Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STUDENT STARTS HERE                       â”‚
â”‚                                                              â”‚
â”‚         python my-orlm/run_training_pipeline.py              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Environment Setup (5-10 minutes)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  setup_environment.py                              â”‚     â”‚
â”‚  â”‚  â€¢ Check GPU availability                          â”‚     â”‚
â”‚  â”‚  â€¢ Install PyTorch + CUDA                          â”‚     â”‚
â”‚  â”‚  â€¢ Install QLoRA dependencies                      â”‚     â”‚
â”‚  â”‚  â€¢ Verify installation                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  Output: âœ“ Ready to train                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: QLoRA Fine-tuning (1-3 hours)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  train_qlora.py                                    â”‚     â”‚
â”‚  â”‚  Input:                                            â”‚     â”‚
â”‚  â”‚    â€¢ Base model: Qwen/Qwen2.5-7B-Instruct         â”‚     â”‚
â”‚  â”‚    â€¢ Data: OR-Instruct-Data-3K-Gurobipy.jsonl     â”‚     â”‚
â”‚  â”‚  Process:                                          â”‚     â”‚
â”‚  â”‚    â€¢ Load model in 4-bit quantization             â”‚     â”‚
â”‚  â”‚    â€¢ Apply LoRA adapters                          â”‚     â”‚
â”‚  â”‚    â€¢ Train on OR problems                         â”‚     â”‚
â”‚  â”‚    â€¢ Save adapter weights                         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  Output: LoRA Adapter (~400MB)                              â”‚
â”‚  Location: ORLM/checkpoints/orlm-qwen3-8b-qlora/           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Merge LoRA Adapter (5-10 minutes)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  merge_model.py                                    â”‚     â”‚
â”‚  â”‚  Input:                                            â”‚     â”‚
â”‚  â”‚    â€¢ Base model: Qwen/Qwen2.5-7B-Instruct         â”‚     â”‚
â”‚  â”‚    â€¢ LoRA adapter (from Step 2)                   â”‚     â”‚
â”‚  â”‚  Process:                                          â”‚     â”‚
â”‚  â”‚    â€¢ Load base model                              â”‚     â”‚
â”‚  â”‚    â€¢ Load adapter weights                         â”‚     â”‚
â”‚  â”‚    â€¢ Merge into single model                      â”‚     â”‚
â”‚  â”‚    â€¢ Save full model                              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  Output: Full Merged Model (~15GB)                          â”‚
â”‚  Location: ORLM/checkpoints/orlm-qwen3-8b-merged/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Test Inference (2-5 minutes)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  test_inference.py                                 â”‚     â”‚
â”‚  â”‚  Input:                                            â”‚     â”‚
â”‚  â”‚    â€¢ Merged model (from Step 3)                   â”‚     â”‚
â”‚  â”‚  Process:                                          â”‚     â”‚
â”‚  â”‚    â€¢ Load merged model                            â”‚     â”‚
â”‚  â”‚    â€¢ Test with sample OR problems                 â”‚     â”‚
â”‚  â”‚    â€¢ Generate solutions                           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  Output: âœ“ Model works correctly                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Convert to GGUF (10-20 minutes)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  convert_to_gguf.py                                â”‚     â”‚
â”‚  â”‚  Input:                                            â”‚     â”‚
â”‚  â”‚    â€¢ Merged model (from Step 3)                   â”‚     â”‚
â”‚  â”‚  Process:                                          â”‚     â”‚
â”‚  â”‚    â€¢ Clone llama.cpp (if needed)                  â”‚     â”‚
â”‚  â”‚    â€¢ Convert to GGUF format (FP16)                â”‚     â”‚
â”‚  â”‚    â€¢ Quantize to Q5_K_M (5-bit)                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  Output: Quantized GGUF (~5GB)                              â”‚
â”‚  Location: ORLM/checkpoints/gguf/model-Q5_K_M.gguf         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: Deploy to Ollama (2-5 minutes)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  deploy_ollama.py                                  â”‚     â”‚
â”‚  â”‚  Input:                                            â”‚     â”‚
â”‚  â”‚    â€¢ GGUF file (from Step 5)                      â”‚     â”‚
â”‚  â”‚  Process:                                          â”‚     â”‚
â”‚  â”‚    â€¢ Create Ollama Modelfile                      â”‚     â”‚
â”‚  â”‚    â€¢ Register model with Ollama                   â”‚     â”‚
â”‚  â”‚    â€¢ Test deployment                              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  Output: âœ“ Model deployed to Ollama                         â”‚
â”‚  Name: orlm-qwen3-8b                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    READY TO USE! ğŸ‰                          â”‚
â”‚                                                              â”‚
â”‚              ollama run orlm-qwen3-8b                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## File Size Progression

```
Original Base Model (Download)
    ğŸ”½ Qwen/Qwen2.5-7B-Instruct (~15GB, cached by HuggingFace)

Training (Step 2)
    ğŸ”½ Creates LoRA Adapter (~400MB)
    ğŸ“ ORLM/checkpoints/orlm-qwen3-8b-qlora/

Merging (Step 3)
    ğŸ”½ Combines into Full Model (~15GB)
    ğŸ“ ORLM/checkpoints/orlm-qwen3-8b-merged/

GGUF Conversion (Step 5)
    ğŸ”½ Converts + Quantizes (~5-6GB)
    ğŸ“ ORLM/checkpoints/gguf/model-Q5_K_M.gguf

Ollama Deployment (Step 6)
    ğŸ”½ Registers with Ollama (no copy, uses GGUF)
    ğŸš€ Ready to use!
```

## Time Breakdown

```
Step 1: Setup           â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  5-10 min  (one-time)
Step 2: Training        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1-3 hours (main time)
Step 3: Merge           â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  5-10 min
Step 4: Test            â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  2-5 min
Step 5: GGUF Convert    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  10-20 min
Step 6: Ollama Deploy   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  2-5 min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                  ~2-4 hours (mostly training)
```

## Disk Space Requirements

```
Required Space:
â”œâ”€â”€ Base Model Cache:        ~15 GB  (HuggingFace cache)
â”œâ”€â”€ LoRA Adapter:            ~0.4 GB (Step 2 output)
â”œâ”€â”€ Merged Model:            ~15 GB  (Step 3 output)
â”œâ”€â”€ GGUF Files:              ~5-6 GB (Step 5 output)
â””â”€â”€ Working Space:           ~5 GB   (temporary files)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                       ~40-45 GB

Optional: After deployment, you can delete:
â”œâ”€â”€ Merged Model (15 GB) - keep GGUF instead
â””â”€â”€ Working files (5 GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Minimum to keep:             ~20-25 GB
```

## GPU Memory Usage

```
Step 2 (Training):
    â€¢ Peak VRAM: ~20-24 GB
    â€¢ Minimum recommended: 24 GB (RTX 3090, A100, etc.)
    â€¢ 4-bit quantization keeps memory low

Step 3 (Merge):
    â€¢ Peak VRAM: ~18 GB
    â€¢ Uses FP16 precision

Step 4 (Test):
    â€¢ Peak VRAM: ~16 GB
    â€¢ Inference mode (less memory)

Steps 5-6:
    â€¢ Mostly CPU operations
    â€¢ Minimal GPU memory
```

## Alternative Paths

### Quick Test Path (Skip Deployment)
```
Step 1 â†’ Step 2 â†’ Step 3 â†’ Step 4
                           STOP HERE
                           (Test only, no Ollama)
```

### Resume from Training
```
Start Here (Skip Step 1 & 2)
     â†“
Step 3 â†’ Step 4 â†’ Step 5 â†’ Step 6
(Use existing adapter)
```

### Custom Parameters Path
```
Step 1 (normal)
     â†“
Step 2 (with custom parameters)
  --epochs 2
  --batch_size 2
  --learning_rate 1e-4
     â†“
Step 3 â†’ 4 â†’ 5 â†’ 6 (normal)
```

## Script Dependencies

```
run_training_pipeline.py (orchestrator)
    â”‚
    â”œâ”€â†’ setup_environment.py
    â”‚       â””â”€â†’ installs: torch, transformers, peft, etc.
    â”‚
    â”œâ”€â†’ train_qlora.py
    â”‚       â”œâ”€â†’ requires: GPU, training data
    â”‚       â””â”€â†’ produces: LoRA adapter
    â”‚
    â”œâ”€â†’ merge_model.py
    â”‚       â”œâ”€â†’ requires: base model, adapter
    â”‚       â””â”€â†’ produces: merged model
    â”‚
    â”œâ”€â†’ test_inference.py
    â”‚       â”œâ”€â†’ requires: merged model
    â”‚       â””â”€â†’ validates: model quality
    â”‚
    â”œâ”€â†’ convert_to_gguf.py
    â”‚       â”œâ”€â†’ requires: merged model, llama.cpp
    â”‚       â””â”€â†’ produces: GGUF file
    â”‚
    â””â”€â†’ deploy_ollama.py
            â”œâ”€â†’ requires: GGUF file, Ollama
            â””â”€â†’ produces: deployed model
```

## Error Recovery

```
If Training Fails:
    â†’ Check GPU memory
    â†’ Reduce batch_size
    â†’ Resume: python train_qlora.py

If Merge Fails:
    â†’ Check adapter exists
    â†’ Check base model access
    â†’ Resume: python merge_model.py

If GGUF Fails:
    â†’ Check llama.cpp installation
    â†’ Retry conversion
    â†’ Resume: python convert_to_gguf.py

If Ollama Fails:
    â†’ Check Ollama installed
    â†’ Install: curl -fsSL https://ollama.com/install.sh | sh
    â†’ Resume: python deploy_ollama.py
```

## Success Indicators

```
âœ“ Step 1: "All dependencies installed successfully!"
âœ“ Step 2: "Training Complete! Model saved to: ..."
âœ“ Step 3: "Merge Complete! Merged model saved to: ..."
âœ“ Step 4: "Model loaded successfully"
âœ“ Step 5: "Conversion Complete! GGUF file: ..."
âœ“ Step 6: "Deployment Complete! Model name: orlm-qwen3-8b"

Final Test:
$ ollama run orlm-qwen3-8b
âœ“ Model responds to prompts â†’ SUCCESS! ğŸ‰
```

## Quick Reference Commands

```bash
# Complete pipeline (recommended)
python my-orlm/run_training_pipeline.py

# Resume from specific step
python my-orlm/run_training_pipeline.py --skip-setup --skip-training

# Individual scripts
python my-orlm/setup_environment.py      # Step 1
python my-orlm/train_qlora.py           # Step 2
python my-orlm/merge_model.py           # Step 3
python my-orlm/test_inference.py        # Step 4
python my-orlm/convert_to_gguf.py       # Step 5
python my-orlm/deploy_ollama.py         # Step 6

# Use the model
ollama run orlm-qwen3-8b

# Get help
python my-orlm/run_training_pipeline.py --help
```

---

**Remember**: The automated pipeline handles all of this for you! Just run:
```bash
python my-orlm/run_training_pipeline.py
```

And sit back while it completes all steps automatically! â˜•
